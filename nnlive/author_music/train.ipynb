{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguish author-specific patterns in music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find this notebook at `EpyNN/nnlive/author_music/train.ipynb`.\n",
    "* Regular python code at `EpyNN/nnlive/author_music/train.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will review:\n",
    "\n",
    "* Handling univariate time series that represents a **huge amount of data points**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow [this link](prepare_dataset.ipynb) for details about data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly, raw data are acoustic guittare music from the *True* author and the *False* author. These are raw ``.wav`` files that were resampled, clipped and digitalized using a 4-bits encoder.\n",
    "\n",
    "Commonly, music ``.wav`` files have a sampling rate of 44100 Hz. This means that each second of music represents a numerical time series of length 44100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EpyNN/nnlive/author_music/train.ipynb\n",
    "# Standard library imports\n",
    "import random\n",
    "\n",
    "# Related third party imports\n",
    "import numpy as np\n",
    "\n",
    "# Local application/library specific imports\n",
    "import nnlibs.initialize\n",
    "from nnlibs.commons.maths import relu, softmax\n",
    "from nnlibs.commons.library import (\n",
    "    configure_directory,\n",
    "    read_model,\n",
    ")\n",
    "from nnlibs.network.models import EpyNN\n",
    "from nnlibs.embedding.models import Embedding\n",
    "from nnlibs.rnn.models import RNN\n",
    "# from nnlibs.lstm.models import LSTM\n",
    "from nnlibs.gru.models import GRU\n",
    "from nnlibs.flatten.models import Flatten\n",
    "from nnlibs.dropout.models import Dropout\n",
    "from nnlibs.dense.models import Dense\n",
    "from prepare_dataset import prepare_dataset\n",
    "from settings import se_hPars\n",
    "\n",
    "\n",
    "########################## CONFIGURE ##########################\n",
    "random.seed(1)\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "np.seterr(all='warn')\n",
    "np.seterr(under='ignore')\n",
    "\n",
    "\n",
    "############################ DATASET ##########################\n",
    "X_features, Y_label = prepare_dataset(N_SAMPLES=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[10  9  9 ...  9  9  9]\n",
      "2 14\n"
     ]
    }
   ],
   "source": [
    "print(len(X_features))\n",
    "print(X_features[0])\n",
    "print(np.min(X_features[0]), np.max(X_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to resampling and clipping, data have been normalized and rewritten on a 4-bits encoder using 16 bins. Audio files are typically 16-bits data or a sequence of integers ranging from 0 to 32767 included. Herein, we will apply one-hot encoding to the input data, so the vocabulary size will be 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward (FF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by our reference, a Feed-Forward network with dropout regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode both features and label and set a batch size of 32.\n",
    "\n",
    "Note that we could, alternatively, not apply ``X_encode`` but ``X_scale`` instead. That would reduce the number of data points but would also consider the amplitude of the signal along with the frequencies it contains.\n",
    "\n",
    "Because each data point turns into an array of shape ``(256,)`` and sum ``1`` upon one-hot encoding, the direct information about amplitude is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(X_data=X_features,\n",
    "                      Y_data=Y_label,\n",
    "                      X_encode=True,\n",
    "                      Y_encode=True,\n",
    "                      batch_size=32,\n",
    "                      relative_size=(2, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667, 44100, 16)\n"
     ]
    }
   ],
   "source": [
    "print(embedding.dtrain.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with the network design and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten-(Dense)n with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We place two *dropout* layers with a ``keep_prob`` of 0.5 each to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'Flatten_Dropout05_Dense-64-relu_Dropout-05_Dense-2-softmax'\n",
    "\n",
    "se_hPars['learning_rate'] = 0.005\n",
    "se_hPars['softmax_temperature'] = 5\n",
    "\n",
    "flatten = Flatten()\n",
    "\n",
    "dropout1 = Dropout(keep_prob=0.5)\n",
    "\n",
    "hidden_dense = Dense(64, relu)\n",
    "\n",
    "dropout2 = Dropout(keep_prob=0.5)\n",
    "\n",
    "dense = Dense(2, softmax)\n",
    "\n",
    "layers = [embedding, flatten, dropout1, hidden_dense, dropout2, dense]\n",
    "\n",
    "model = EpyNN(layers=layers, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We have set the softmax temperature to ``5`` to diminish the confidence of the model and the risk of vanishing/exploding gradients.\n",
    "\n",
    "We can initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m--- EpyNN Check --- \u001b[0m\n",
      "\u001b[1mLayer: Embedding\u001b[0m\n",
      "\u001b[1m\u001b[32mcompute_shapes: Embedding\u001b[0m\n",
      "\u001b[1m\u001b[32minitialize_parameters: Embedding\u001b[0m\n",
      "\u001b[1m\u001b[32mforward: Embedding\u001b[0m\n",
      "\u001b[1mLayer: Flatten\u001b[0m\n",
      "\u001b[1m\u001b[32mcompute_shapes: Flatten\u001b[0m\n",
      "\u001b[1m\u001b[32minitialize_parameters: Flatten\u001b[0m\n",
      "\u001b[1m\u001b[32mforward: Flatten\u001b[0m\n",
      "\u001b[1mLayer: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[32mcompute_shapes: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[32minitialize_parameters: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[32mforward: Dropout\u001b[0m\n",
      "\u001b[1mLayer: Dense\u001b[0m\n",
      "\u001b[1m\u001b[32mcompute_shapes: Dense\u001b[0m\n",
      "\u001b[1m\u001b[32minitialize_parameters: Dense\u001b[0m\n",
      "\u001b[1m\u001b[32mforward: Dense\u001b[0m\n",
      "\u001b[1mLayer: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[32mcompute_shapes: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[32minitialize_parameters: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[32mforward: Dropout\u001b[0m\n",
      "\u001b[1mLayer: Dense\u001b[0m\n",
      "\u001b[1m\u001b[32mcompute_shapes: Dense\u001b[0m\n",
      "\u001b[1m\u001b[32minitialize_parameters: Dense\u001b[0m\n",
      "\u001b[1m\u001b[32mforward: Dense\u001b[0m\n",
      "\u001b[1mLayer: Dense\u001b[0m\n",
      "\u001b[1m\u001b[36mbackward: Dense\u001b[0m\n",
      "\u001b[1m\u001b[36mcompute_gradients: Dense\u001b[0m\n",
      "\u001b[1mLayer: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[36mbackward: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[36mcompute_gradients: Dropout\u001b[0m\n",
      "\u001b[1mLayer: Dense\u001b[0m\n",
      "\u001b[1m\u001b[36mbackward: Dense\u001b[0m\n",
      "\u001b[1m\u001b[36mcompute_gradients: Dense\u001b[0m\n",
      "\u001b[1mLayer: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[36mbackward: Dropout\u001b[0m\n",
      "\u001b[1m\u001b[36mcompute_gradients: Dropout\u001b[0m\n",
      "\u001b[1mLayer: Flatten\u001b[0m\n",
      "\u001b[1m\u001b[36mbackward: Flatten\u001b[0m\n",
      "\u001b[1m\u001b[36mcompute_gradients: Flatten\u001b[0m\n",
      "\u001b[1mLayer: Embedding\u001b[0m\n",
      "\u001b[1m\u001b[36mbackward: Embedding\u001b[0m\n",
      "\u001b[1m\u001b[36mcompute_gradients: Embedding\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.initialize(loss='BCE', seed=1, metrics=['accuracy', 'recall', 'precision'], se_hPars=se_hPars.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(epochs=50, init_logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the attention we paid to prevent overfitting, we observe that the model has well reproduced the training data while it fails to achieve a comparable accuracy on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding setup is the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.51 GiB for an array with shape (667, 44100, 16) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f9d107ce12cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                       \u001b[0mY_encode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                       relative_size=(2, 1, 0))\n\u001b[0m",
      "\u001b[0;32m/media/synthase/beta/EpyNN/nnlibs/embedding/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_data, Y_data, relative_size, batch_size, X_encode, Y_encode, X_scale)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_encode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_encode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0membedded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/synthase/beta/EpyNN/nnlibs/embedding/dataset.py\u001b[0m in \u001b[0;36membedding_prepare\u001b[0;34m(layer, X_data, Y_data)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdval\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dtrain'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dtest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mdval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/synthase/beta/EpyNN/nnlibs/commons/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_data, Y_data, name)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# Vectorize X_data in NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Set of sample label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 3.51 GiB for an array with shape (667, 44100, 16) and data type float64"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(X_data=X_features,\n",
    "                      Y_data=Y_label,\n",
    "                      X_encode=True,\n",
    "                      Y_encode=True,\n",
    "                      batch_size=32,\n",
    "                      relative_size=(2, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN(sequences=True)-Flatten-Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an even greater softmax temperature since the *RNN* cell is very sensitive to the problem of exploding/vanishing gradient.\n",
    "\n",
    "We have also set the ``sequences=True`` flag, which means that the *RNN* layer will forward all hidden cell states to the next layer, and not only the last one computed in the 100th cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'RNN-100-Seq_Flatten_Dense-2-softmax'\n",
    "\n",
    "se_hPars['learning_rate'] = 0.05\n",
    "se_hPars['softmax_temperature'] = 10\n",
    "\n",
    "rnn = RNN(16, sequences=True)\n",
    "\n",
    "flatten = Flatten()\n",
    "\n",
    "dense = Dense(2, softmax)\n",
    "\n",
    "layers = [embedding, rnn, flatten, dense]\n",
    "\n",
    "model = EpyNN(layers=layers, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.initialize(loss='BCE', seed=1, metrics=['accuracy', 'recall', 'precision'], se_hPars=se_hPars.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(epochs=5, init_logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again it seems that the network did reproduce the training data well, but the metrics on the testing set did not improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU(sequences=True)-Flatten-Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'GRU-100-Seq_Flatten_Dense-2-softmax'\n",
    "\n",
    "se_hPars['learning_rate'] = 0.05\n",
    "se_hPars['softmax_temperature'] = 10\n",
    "\n",
    "gru = GRU(16)\n",
    "\n",
    "flatten = Flatten()\n",
    "\n",
    "dense = Dense(2, softmax)\n",
    "\n",
    "layers = [embedding, gru, dense]\n",
    "\n",
    "model = EpyNN(layers=layers, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Initialize and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.initialize(loss='BCE', seed=1, metrics=['accuracy', 'recall', 'precision'], se_hPars=se_hPars.copy())\n",
    "\n",
    "model.train(epochs=10, init_logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
